{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 2: classifiy adjectives in reviews as positive or negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import librarys and prepare data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# librarys\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "import nltk.sentiment\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.sentiment\n",
    "import string\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from sklearn import linear_model\n",
    "import collections\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "import multiprocessing\n",
    "import time\n",
    "import inflect\n",
    "from textblob import TextBlob\n",
    "import keras.preprocessing.text as T\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "## define stopwords\n",
    "sr = stopwords.words('english')\n",
    "p = inflect.engine()\n",
    "wnl = WordNetLemmatizer() \n",
    "table = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to change text to words\n",
    "def clean_review(text):\n",
    "    #Change n't into not\n",
    "    x = re.sub(r'n\\'t',' not',text)\n",
    "    #Change not word into not_word\n",
    "    x = re.sub(r'not ','not_',x)\n",
    "    #Split into words\n",
    "    x = word_tokenize(x)\n",
    "    #Remove punctuation\n",
    "    x = [w.translate(table) if not re.match(r'not_.*', w) else w for w in x]\n",
    "    #Change numbers into words\n",
    "    x = [p.number_to_words(w) if w.isdigit() else w  for w in x ]\n",
    "    #Remove non alphabetic\n",
    "    x = [w for w in x if w.isalpha() or re.match(r'not_.*',w)]\n",
    "    #Convert to lower case\n",
    "    x = [w.lower() for w in x]\n",
    "    #Remove stop words\n",
    "    x = [w for w in x if not w in sr]\n",
    "    ## lemmatization\n",
    "    x = [wnl.lemmatize(w) for w in x]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nsNONDHbV7Vudqh21uicqw</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Was very excited for happy hour and heard grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nsNONDHbV7Vudqh21uicqw</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Seems old and tired!   I ate here frequently a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nsNONDHbV7Vudqh21uicqw</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Poor service, had to sit there and ask my wife...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nsNONDHbV7Vudqh21uicqw</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yesterday I took my husband to eat on his birt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nsNONDHbV7Vudqh21uicqw</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Second and last time. Sadly I had to wait for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  stars  \\\n",
       "0  nsNONDHbV7Vudqh21uicqw    1.0   \n",
       "1  nsNONDHbV7Vudqh21uicqw    2.0   \n",
       "2  nsNONDHbV7Vudqh21uicqw    1.0   \n",
       "3  nsNONDHbV7Vudqh21uicqw    1.0   \n",
       "4  nsNONDHbV7Vudqh21uicqw    1.0   \n",
       "\n",
       "                                                text  \n",
       "0  Was very excited for happy hour and heard grea...  \n",
       "1  Seems old and tired!   I ate here frequently a...  \n",
       "2  Poor service, had to sit there and ask my wife...  \n",
       "3  Yesterday I took my husband to eat on his birt...  \n",
       "4  Second and last time. Sadly I had to wait for ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "review=pd.read_csv(\"review_with_useful_features.csv\")\n",
    "review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Was very excited for happy hour and heard grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>Seems old and tired!   I ate here frequently a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Poor service, had to sit there and ask my wife...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Yesterday I took my husband to eat on his birt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>Second and last time. Sadly I had to wait for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      stars                                               text\n",
       "0  negative  Was very excited for happy hour and heard grea...\n",
       "1  negative  Seems old and tired!   I ate here frequently a...\n",
       "2  negative  Poor service, had to sit there and ask my wife...\n",
       "3  negative  Yesterday I took my husband to eat on his birt...\n",
       "4  negative  Second and last time. Sadly I had to wait for ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare data\n",
    "review=review.drop(columns=['business_id'])\n",
    "# we define stars from 1 to 3 as negative and stars from 4 to 5 as positive\n",
    "review['stars']=np.where(review['stars'] >= 4, \"positive\", \"negative\")\n",
    "review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract adjectives from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do word tokenization\n",
    "clean_text = [[1]]*len(review)\n",
    "for i in range(len(review)):\n",
    "    clean_text[i]=clean_review(review['text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get adjectives from all words\n",
    "adj_words = [[1]]*len(review)\n",
    "for i in range(len(review)):\n",
    "    word=clean_text[i]\n",
    "    pos_tag = nltk.pos_tag(word)\n",
    "    adjs = []\n",
    "    for words, pos in pos_tag:\n",
    "        if (pos == 'JJ'):\n",
    "            adjs.append(words)\n",
    "    adj_words[i]=adjs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count frequencies for adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1325513"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all adjectives in one list\n",
    "alladjwords = []\n",
    "for i in range(len(review)):\n",
    "    alladjwords=alladjwords + adj_words[i]\n",
    "len(alladjwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of adjectives in all reviews is 29050\n"
     ]
    }
   ],
   "source": [
    "# count the frequency for each adjectives\n",
    "frequency = collections.Counter(alladjwords)\n",
    "dict_frequency = dict(frequency)\n",
    "print('the total number of adjectives in all reviews is',len(dict_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['good', 'great', 'delicious', 'fresh', 'fish', 'nice', 'little', 'much', 'special', 'small', 'first', 'hot', 'new', 'next', 'favorite', 'bad', 'many', 'last', 'excellent', 'big', 'chinese', 'happy', 'sure', 'different', 'friendly', 'overall', 'full', 'large', 'worth', 'busy', 'awesome', 'huge', 'right', 'tasty', 'sweet', 'red', 'vega', 'restaurant', 'top', 'high', 'cheese', 'attentive', 'green', 'perfect', 'whole', 'fantastic', 'clean', 'long', 'open', 'garlic', 'italian', 'ok', 'amazing', 'mexican', 'second', 'extra', 'authentic', 'wonderful', 'old', 'super', 'hard', 'main', 'several', 'wrong', 'disappointed', 'fifteen', 'expensive', 'decent', 'chicken', 'entree', 'quick', 'free', 'white', 'fried', 'regular', 'meal', 'real', 'flavorful', 'japanese', 'reasonable', 'enough', 'cold', 'crawfish', 'lunch', 'ten', 'soft', 'slow', 'able', 'come', 'wait', 'dry', 'ordered', 'vegetable', 'black', 'wish', 'not_a', 'local', 'live', 'horrible', 'empty', 'entire', 'raw', 'fine', 'warm', 'average', 'short', 'menu', 'asian', 'terrible', 'outstanding', 'beautiful', 'hungry', 'minute', 'eat', 'dessert', 'tried', 'ready', 'light', 'helpful', 'generous', 'tiny', 'recommend', 'available', 'late', 'bean', 'close', 'early', 'not_want', 'not_the', 'prepared', 'not_know', 'olive', 'incredible', 'poor', 'sour', 'sandwich', 'fry', 'mixed', 'french', 'unique', 'due', 'simple', 'cheap', 'taco', 'thai', 'salmon', 'front', 'typical', 'cool', 'easy', 'impressed', 'not_have', 'usual', 'outside', 'traditional', 'le', 'rich', 'sum', 'wine', 'saturday', 'couple', 'want', 'friday', 'catfish', 'total', 'casual', 'low', 'deep', 'pricey', 'frozen', 'combo', 'twenty', 'half', 'pleasant', 'lemon', 'disappointing', 'similar', 'ask', 'solid', 'crab', 'sunday', 'clam', 'single', 'creamy', 'blue', 'server', 'fabulous', 'loud', 'bite', 'side', 'ate', 'lobster', 'true', 'comfortable', 'mean', 'n', 'try', 'heavy', 'fast', 'tough', 'popular', 'complimentary', 'soup', 'thirty', 'sauce', 'steak', 'sat', 'atmosphere', 'awful', 'oh', 'knowledgeable', 'not_too', 'grilled', 'finish', 'know', 'sad', 'standard', 'salsa', 'strong', 'american', 'vegetarian', 'fair', 'uni', 'normal', 'enjoyed', 'rare', 'exceptional', 'weird', 'thin', 'glad', 'not_been', 'general', 'original', 'okay', 'dim', 'modern', 'not_worth', 'middle', 'negative', 'wanted', 'crazy', 'young', 'affordable', 'greek', 'pretty', 'scallop', 'actual', 'fifty', 'difficult', 'past', 'sticky', 'clear', 'greasy', 'phenomenal', 'gumbo', 'enjoyable', 'calamari', 'sick', 'previous', 'multiple', 'plenty', 'classic', 'delish', 'joint', 'eggplant', 'gross', 'possible', 'soggy', 'not_much', 'choose', 'puppy', 'appetizer', 'yummy', 'buffet', 'third', 'mild', 'spectacular', 'east', 'like', 'lucky', 'write', 'quiet', 'positive', 'not_to', 'sashimi', 'korean', 'thick', 'pasta', 'yellow', 'personal', 'dirty', 'tofu', 'professional', 'interior', 'fancy', 'additional', 'friend', 'surprised', 'pick', 'impressive', 'enjoy', 'miss', 'strip', 'recent', 'basic', 'ridiculous', 'eighteen', 'willing', 'interesting', 'arrive', 'la', 'hidden', 'upscale', 'prime', 'stick', 'english', 'san', 'healthy', 'tip', 'cooked', 'daily', 'not_like', 'snow', 'stop', 'liked', 'rude', 'certain', 'flat', 'wide', 'various', 'indian', 'consistent', 'not_come', 'crispy', 'brown', 'particular', 'romantic', 'phoenix', 'bottle', 'birthday', 'memorable', 'funny', 'odd', 'plate', 'fishy', 'pan', 'told', 'fat', 'picky', 'type', 'drink', 'double', 'cajun', 'love', 'sixteen', 'efficient', 'make', 'sorry', 'not_give', 'limited', 'famous', 'mouth', 'nigiri', 'anniversary', 'octopus', 'not_bad', 'wild', 'fourteen', 'favourite', 'flavour', 'separate', 'complete', 'important', 'not_for', 'lady', 'satisfied', 'courteous', 'eaten', 'potato', 'wow', 'thought', 'impeccable', 'polite', 'private', 'spice', 'tuna', 'extensive', 'nobu', 'rice', 'welcome', 'drive', 'delightful', 'place', 'cute', 'dark', 'southern', 'stuffed', 'pleased', 'cantonese', 'vietnamese', 'seasoned', 'forty', 'touch', 'pad', 'flavor', 'hawaiian', 'key', 'spacious', 'not_fresh', 'not_expect', 'north', 'likely', 'fun', 'smile', 'veggie', 'tuesday', 'guess', 'excited', 'strange', 'jumbo', 'mary', 'grand', 'nasty', 'not_as', 'messy', 'spicy', 'need', 'patio', 'salt', 'downtown', 'hostess', 'salty', 'not_even', 'creative', 'tomato', 'plastic', 'add', 'bring', 'mongolian', 'major', 'ayce', 'loved', 'waiter', 'lime', 'ocean', 'piece', 'thirteen', 'lol', 'udon', 'recommended', 'alaskan', 'not_be', 'chose', 'update', 'margarita', 'take', 'frequent', 'parmesan', 'future', 'uncomfortable', 'waited', 'not_busy', 'establishment', 'carne', 'south', 'tonight', 'lamb', 'decor', 'secret', 'giant', 'avoid', 'bunch', 'inside', 'ta', 'horseradish', 'personable', 'tea', 'shellfish', 'brought', 'hear', 'not_care', 'angry', 'prompt', 'numerous', 'guacamole', 'thursday', 'serious', 'outdoor', 'chinatown', 'monday', 'mushy', 'bright', 'thank', 'wednesday', 'dead', 'gorgeous', 'noticed', 'seat', 'spanish', 'everyday', 'edible', 'plentiful', 'peruvian', 'straight', 'balsamic', 'bottom', 'not_go', 'inedible', 'star', 'mediocre', 'ive', 'mussel', 'pho', 'bit', 'name', 'w', 'tasted', 'cafe', 'ingredient', 'read', 'twelve', 'start', 'familiar', 'flavourful', 'duck', 'seventeen', 'golden', 'left', 'understand', 'not_on', 'final', 'speak', 'seated', 'dinner', 'kitchen', 'quality', 'seasonal', 'upset', 'common', 'nacho', 'pizza', 'not_my', 'careful', 'kick', 'weekday', 'safe', 'inexpensive', 'call', 'not_just', 'interested', 'undercooked', 'adventurous', 'eric', 'massive', 'ice', 'unbelievable', 'saw', 'baked', 'soy', 'aware', 'obvious', 'not_in', 'door', 'tasteless', 'pro', 'skeptical', 'nearby', 'fatty', 'comparable', 'find', 'foie', 'chef', 'go', 'scottsdale', 'napkin', 'guy', 'eating', 'iced', 'else', 'lb', 'packed', 'stellar', 'chewy', 'culinary', 'sit', 'absolute', 'not_get', 'fill', 'end', 'not_so', 'omg', 'min', 'purple', 'correct', 'stuff', 'swordfish', 'see', 'overpriced', 'return', 'spent', 'refill', 'pete', 'individual', 'ny', 'delectable', 'level', 'keep', 'girl', 'not_sure', 'dine', 'elegant', 'asparagus', 'asked', 'leave', 'not_eat', 'spend', 'acceptable', 'steamed', 'expect', 'specific', 'forgot', 'flaky', 'exact', 'pm', 'meat', 'spare', 'steep', 'mediterranean', 'fix', 'utensil', 'weak', 'online', 'impossible', 'gaucho', 'cant', 'minimal', 'fit', 'tamale', 'overcooked', 'crappy', 'walked', 'let', 'mahi', 'mine', 'burnt', 'smooth', 'proper', 'pepper', 'diet', 'present', 'roll', 'lot', 'nicky', 'back', 'brunch', 'ordinary', 'appreciate', 'sample', 'urchin', 'bitter', 'unprofessional', 'not_miss', 'agree', 'found', 'worthy', 'natural', 'needle', 'scrumptious', 'not_quite', 'ample', 'worst', 'die', 'crunchy', 'unusual', 'bbq', 'enchilada', 'fan', 'curry', 'receive', 'dive', 'trendy', 'rosemary', 'alternative', 'visit', 'save', 'miso', 'un', 'nineteen', 'stale', 'slight', 'pineapple', 'review', 'squid', 'ultimate', 'homemade', 'umiya', 'dont', 'mac', 'timely', 'prior', 'avocado', 'knowledgable', 'savory', 'buck', 'pecan', 'classy', 'venetian', 'suppose', 'alright', 'desert', 'weekly', 'terrific', 'complex', 'awhile', 'not_live', 'ahi', 'unacceptable', 'not_your', 'cellar', 'gotten', 'tight', 'cozy', 'minimum', 'arizona', 'fellow', 'unlimited', 'central', 'appropriate', 'potential', 'stupid', 'non', 'upgrade', 'bland', 'skin', 'not_an', 'ew', 'informative', 'not_take', 'wall', 'not_do', 'bolognese', 'crowd', 'cup', 'omakase', 'added', 'get', 'website', 'initial', 'shot', 'raku', 'chive', 'skip', 'gracious', 'gravy', 'luke', 'mix', 'not_happy', 'legit', 'extraordinary', 'corporate', 'par', 'noisy', 'stuck', 'veal', 'cart', 'valentine', 'bowl', 'not_make', 'lasagna', 'garnish', 'window', 'bag', 'mango', 'sixty', 'curious', 'delicate', 'service', 'peak', 'endless', 'patient', 'native', 'sound', 'premium', 'female', 'not_recommend', 'quite', 'not_cheap', 'minor', 'west', 'pretentious', 'bun', 'social', 'spoonful', 'appetite', 'not_able', 'mad', 'wasabi', 'confused', 'st', 'radish', 'suck', 'rest', 'current', 'kong', 'afraid', 'ravioli', 'celebrate', 'heaven', 'not_disappoint', 'boyfriend', 'canadian', 'crisp', 'knew', 'tom', 'sloppy', 'lazy', 'sign', 'superior', 'unhappy', 'surprising', 'wet', 'used', 'not_seem', 'pas', 'twice', 'handful', 'former', 'upstairs', 'subtle', 'leg', 'freshness', 'apologetic', 'togo', 'ton', 'exterior', 'tired', 'hate', 'hong', 'not_finish', 'necessary', 'chilean', 'prepare', 'not_think', 'awkward', 'queen', 'halibut', 'mall', 'neat', 'vibe', 'disappointment', 'served', 'not_any', 'colorful', 'coconut', 'beef', 'im', 'decided', 'caesar', 'linguine', 'not_many', 'box', 'truffle', 'annoyed', 'stay', 'broccoli', 'unpleasant', 'please', 'exquisite', 'dimsum', 'hesitant', 'unfortunate', 'chic', 'vegan', 'italy', 'square', 'moist', 'hit', 'enormous', 'highlight', 'not_available', 'arrival', 'grab', 'lean', 'exotic', 'yelp', 'random', 'gluten', 'rush', 'tropical', 'intimate', 'arrived', 'medium', 'alcoholic', 'flour', 'al', 'sake', 'michoacan', 'unable', 'lovely', 'scoop', 'caprese', 'overwhelming', 'foodie', 'british', 'bay', 'cod', 'fourth', 'visited', 'horrendous', 'garlicky', 'tall', 'app', 'rough', 'sichuan', 'cocktail', 'genuine', 'slice', 'checked', 'not_enjoy', 'rushed', 'lack', 'complementary', 'understandable', 'not_greasy', 'pappadeaux', 'happen', 'opposite', 'not_good', 'generic', 'adequate', 'chipotle', 'pittsburgh', 'flavorless', 'nyc', 'pong', 'allergic', 'looked', 'not_hot', 'basil', 'rustic', 'lousy', 'ur', 'meatball', 'winterlicious', 'not_enough', 'unagi', 'az', 'inattentive', 'chili', 'underwhelming', 'rib', 'got', 'roast', 'understaffed', 'convenient', 'experienced', 'notice', 'follow', 'atlantic', 'mile', 'counter', 'etc', 'toronto', 'eager', 'hype', 'louisiana', 'uptown', 'not_see', 'poke', 'concerned', 'didnt', 'fluffy', 'western', 'split', 'public', 'unexpected', 'custard', 'not_at', 'joyful', 'grill', 'wear', 'holy', 'socal', 'eastern', 'yum', 'not_need', 'bacon', 'gigantic', 'underwhelmed', 'taiwanese', 'kid', 'shredded', 'bisque', 'commercial', 'chile', 'rick', 'wendy', 'burrito', 'staple', 'not_very', 'miku', 'starter', 'not_impressed', 'not_ready', 'superb', 'serf', 'bar', 'um', 'francisco', 'relative', 'spiciness', 'maine', 'not_great', 'eclectic', 'swear', 'plain', 'apps', 'eddie', 'greeted', 'decide', 'pour', 'boring', 'alfredo', 'lindo', 'think', 'tasteful', 'list', 'not_mean', 'beat', 'girlfriend', 'lv', 'aburi', 'heard', 'leaf', 'not_', 'leftover', 'not_tried', 'complaint', 'uber', 'cuttlefish', 'handle', 'serve', 'sicilian', 'hushpuppy', 'closed', 'soda', 'hefty', 'god', 'rid', 'mom', 'not_open', 'goto', 'cole', 'broken', 'tart', 'preferred', 'attractive', 'eleven', 'p', 'fav', 'forgettable', 'lackluster', 'edamame', 'intense', 'chocolate', 'spot', 'hey', 'hill', 'lukewarm', 'distinct', 'bang', 'overwhelmed', 'california', 'equal', 'not_seen', 'child', 'adorable', 'tad', 'humble', 'cheek', 'bloody', 'sesame', 'regret', 'addictive', 'not_terrible', 'enthusiastic', 'pathetic', 'started', 'wooden', 'break', 'guilty', 'montreal', 'asada', 'teriyaki', 'feel', 'tangy', 'magical', 'vast', 'item', 'tilapia', 'eighty', 'jamaican', 'portuguese', 'downhill', 'surf', 'outrageous', 'loyal', 'alive', 'senior', 'thumb', 'recipe', 'sister', 'jalapeno', 'foot', 'assortment', 'significant', 'imagine', 'banana', 'formal', 'szechuan', 'valley', 'filthy', 'fake', 'not_speak', 'tempura', 'summary', 'seared', 'boy', 'bc', 'not_disappointed', 'sensitive', 'summerlicious', 'smart', 'hilarious', 'ran', 'fondue', 'questionable', 'attitude', 'agedashi', 'nose', 'ugh', 'unassuming', 'select', 'nervous', 'subpar', 'hospitable', 'unreal', 'meh', 'nicer', 'human', 'shady', 'tap', 'yellowtail', 'lil', 'baja', 'wahoo', 'sketchy', 'pushy', 'ear', 'ranch', 'los', 'naan', 'care', 'tacky', 'oz', 'experience', 'walnut', 'china', 'arctic', 'kept', 'accommodate', 'ethnic', 'thrown', 'everytime', 'andouille', 'ribeye', 'pacific', 'pricier', 'hush', 'crowded', 'pull', 'tempe', 'inconsistent', 'gnocchi', 'not_up', 'definite', 'dip', 'forth'])"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get 1200 most common adjectives and then remove some wrong words\n",
    "removeword=['u','dish','table','shrimp','noodle','sushi','salad','give','seafood','oyster','waitress','bread']\n",
    "common = frequency.most_common(1200)\n",
    "common=dict(common)\n",
    "for word in removeword:\n",
    "    common.pop(word)\n",
    "print(common.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get frequency matrix (row is the index of reviews and column is each adjective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>delicious</th>\n",
       "      <th>fresh</th>\n",
       "      <th>fish</th>\n",
       "      <th>nice</th>\n",
       "      <th>little</th>\n",
       "      <th>much</th>\n",
       "      <th>special</th>\n",
       "      <th>small</th>\n",
       "      <th>...</th>\n",
       "      <th>hush</th>\n",
       "      <th>crowded</th>\n",
       "      <th>pull</th>\n",
       "      <th>tempe</th>\n",
       "      <th>inconsistent</th>\n",
       "      <th>gnocchi</th>\n",
       "      <th>not_up</th>\n",
       "      <th>definite</th>\n",
       "      <th>dip</th>\n",
       "      <th>forth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   good  great  delicious  fresh  fish  nice  little  much  special  small  \\\n",
       "0     0      1          0      0     0     0       0     0        0      0   \n",
       "1     1      1          0      0     0     0       0     0        0      0   \n",
       "2     0      0          0      0     0     0       0     1        0      0   \n",
       "3     0      0          1      0     2     0       0     0        0      0   \n",
       "4     0      0          0      0     0     0       0     0        0      0   \n",
       "\n",
       "   ...    hush  crowded  pull  tempe  inconsistent  gnocchi  not_up  definite  \\\n",
       "0  ...       0        0     0      0             0        0       0         0   \n",
       "1  ...       0        0     0      0             0        0       0         0   \n",
       "2  ...       0        0     0      0             0        0       0         0   \n",
       "3  ...       0        0     0      0             0        0       0         0   \n",
       "4  ...       0        0     0      0             0        0       0         0   \n",
       "\n",
       "   dip  forth  \n",
       "0    0      0  \n",
       "1    0      0  \n",
       "2    0      0  \n",
       "3    0      0  \n",
       "4    0      0  \n",
       "\n",
       "[5 rows x 1188 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get frequency matrix for adjectives\n",
    "corpus=[]\n",
    "corpus=list(review['text'])\n",
    "df = pd.DataFrame(data=corpus, columns=['sentences'])\n",
    "vectorizer = CountVectorizer(vocabulary=list(common.keys()), min_df=0)\n",
    "X = vectorizer.fit_transform(df['sentences'].values)\n",
    "result = pd.DataFrame(data=X.toarray(), columns=vectorizer.get_feature_names())\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use multinomial Naive Bayes model to classify adjectives as positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit multinomial Naive Bayes model\n",
    "Y=review['stars']\n",
    "model = MultinomialNB()\n",
    "model.fit(result, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive']\n",
      "['negative']\n"
     ]
    }
   ],
   "source": [
    "# define classifier function and give two examples\n",
    "def classifier(adjective):\n",
    "    return model.predict(vectorizer.transform([adjective]))\n",
    "print(classifier('great'))\n",
    "print(classifier('bad'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get and save dictionary for adjectives with positive or negative tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dictionary\n",
    "dict_adj={}\n",
    "for word in common.keys():\n",
    "    dict_adj[word]=classifier(word)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary\n",
    "f = open('dict_adj.txt','w')\n",
    "f.write(str(dict_adj))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
