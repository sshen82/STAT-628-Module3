{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import math\n",
    "from math import sqrt\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from sklearn import linear_model\n",
    "import collections\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "import multiprocessing\n",
    "import time\n",
    "import inflect\n",
    "from textblob import TextBlob\n",
    "import keras.preprocessing.text as T\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer #该类会将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "## define stopwords\n",
    "sr = stopwords.words('english')\n",
    "p = inflect.engine()\n",
    "wnl = WordNetLemmatizer() \n",
    "table = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define function \"read_json\" and \"write_json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path):\n",
    "    with open(path, 'r',encoding='utf-8') as f:\n",
    "        iter_f = iter(f)\n",
    "        line = f.readline()\n",
    "        text = []\n",
    "        for line in iter_f: \n",
    "            d = json.loads(line)\n",
    "            print(d)\n",
    "            text.append(d)\n",
    "        f.close()\n",
    "    return(text)\n",
    "    \n",
    "def write_json(path, data):\n",
    "    with open(path, 'w', encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read \"seafood_review.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seafood_review=read_json(\"example.json\")\n",
    "# with open(\"example.json\"，encoding='utf-8') as file:\n",
    "#     result = json.loads(file)\n",
    "# file.close()\n",
    "# seafood_review=json.loads(\"example.json\")\n",
    "with open('example.json', 'r',encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "    json_data = json.loads(data)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_id': '8NBE5q5UWLQUqWyM7GpQ2g',\n",
       " 'user_id': 'StZTVDuFzahNvjl5qu6l7Q',\n",
       " 'business_id': 'nsNONDHbV7Vudqh21uicqw',\n",
       " 'stars': 2.0,\n",
       " 'useful': 1,\n",
       " 'funny': 1,\n",
       " 'cool': 1,\n",
       " 'text': 'Seems old and tired!   I ate here frequently about 20 years ago and returned for happy hour this year.  This used to be a good place many years ago, but too many great restaurants have opened in recent years to eat here.  Strange rules about what you can eat in the bar area.  Food just meh.',\n",
       " 'date': '2018-05-08 17:18:20'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "543"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\GAO\n",
      "[nltk_data]     ZEJIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## review processiong\n",
    "review_train_clean = [[1]]*len(data_train)\n",
    "##a list  with dict element,you can get a summary of frequency of one review by [i] and frequency for a specific word by [i]['word']\n",
    "frequency_train_clean = [[1]]*len(data_train)  \n",
    "for i in tqdm(range(len(data_train))):\n",
    "    #Change n't into not\n",
    "    x = re.sub(r'n\\'t',' not',data_train.iloc[i]['text'])\n",
    "    #Change not adj into not_adj\n",
    "    x = re.sub(r'not ','not_',x)\n",
    "    #Split into words\n",
    "    x = word_tokenize(x)\n",
    "    #Remove punctuation\n",
    "    x = [w.translate(table) if not re.match(r'not_.*', w) else w for w in x]\n",
    "    #Change numbers into words\n",
    "    x = [p.number_to_words(w) if w.isdigit() else w  for w in x ]\n",
    "    #Remove non alphabetic\n",
    "    x = [w for w in x if w.isalpha() or re.match(r'not_.*',w)]\n",
    "    #Convert to lower case\n",
    "    x = [w.lower() for w in x]\n",
    "    #Remove stop words\n",
    "    x = [w for w in x if not w in sr]\n",
    "    ## lemmatization\n",
    "    x = [wnl.lemmatize(w) for w in x]\n",
    "    review_train_clean[i] = x\n",
    "    frequency_train_clean[i] = collections.Counter(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\GAO\n",
      "[nltk_data]     ZEJIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Seems', 'old', 'and', 'tired', '', 'I', 'ate', 'here', 'frequently', 'about', '20', 'years', 'ago', 'and', 'returned', 'for', 'happy', 'hour', 'this', 'year', '', 'This', 'used', 'to', 'be', 'a', 'good', 'place', 'many', 'years', 'ago', '', 'but', 'too', 'many', 'great', 'restaurants', 'have', 'opened', 'in', 'recent', 'years', 'to', 'eat', 'here', '', 'Strange', 'rules', 'about', 'what', 'you', 'can', 'eat', 'in', 'the', 'bar', 'area', '', 'Food', 'just', 'meh', '']\n",
      "['Seems', 'old', 'and', 'tired', '', 'I', 'ate', 'here', 'frequently', 'about', 'twenty', 'years', 'ago', 'and', 'returned', 'for', 'happy', 'hour', 'this', 'year', '', 'This', 'used', 'to', 'be', 'a', 'good', 'place', 'many', 'years', 'ago', '', 'but', 'too', 'many', 'great', 'restaurants', 'have', 'opened', 'in', 'recent', 'years', 'to', 'eat', 'here', '', 'Strange', 'rules', 'about', 'what', 'you', 'can', 'eat', 'in', 'the', 'bar', 'area', '', 'Food', 'just', 'meh', '']\n",
      "['seems', 'old', 'and', 'tired', 'i', 'ate', 'here', 'frequently', 'about', 'twenty', 'years', 'ago', 'and', 'returned', 'for', 'happy', 'hour', 'this', 'year', 'this', 'used', 'to', 'be', 'a', 'good', 'place', 'many', 'years', 'ago', 'but', 'too', 'many', 'great', 'restaurants', 'have', 'opened', 'in', 'recent', 'years', 'to', 'eat', 'here', 'strange', 'rules', 'about', 'what', 'you', 'can', 'eat', 'in', 'the', 'bar', 'area', 'food', 'just', 'meh']\n",
      "['seems', 'old', 'tired', 'ate', 'frequently', 'twenty', 'years', 'ago', 'returned', 'happy', 'hour', 'year', 'used', 'good', 'place', 'many', 'years', 'ago', 'many', 'great', 'restaurants', 'opened', 'recent', 'years', 'eat', 'strange', 'rules', 'eat', 'bar', 'area', 'food', 'meh']\n",
      "['seems', 'old', 'tired', 'ate', 'frequently', 'twenty', 'year', 'ago', 'returned', 'happy', 'hour', 'year', 'used', 'good', 'place', 'many', 'year', 'ago', 'many', 'great', 'restaurant', 'opened', 'recent', 'year', 'eat', 'strange', 'rule', 'eat', 'bar', 'area', 'food', 'meh']\n"
     ]
    }
   ],
   "source": [
    "## example    \n",
    "#     x = \"Worst Gyms I met. And I wouldn't come again 3. not great.\"\n",
    "    x = \"Seems old and tired!   I ate here frequently about 20 years ago and returned for happy hour this year.  This used to be a good place many years ago, but too many great restaurants have opened in recent years to eat here.  Strange rules about what you can eat in the bar area.  Food just meh.\"\n",
    "    p = inflect.engine()\n",
    "    x = re.sub(r'n\\'t',' not',x)\n",
    "    #Change not adj into not_adj\n",
    "    x = re.sub(r'not ','not_',x)\n",
    "    #Split into words\n",
    "    x = word_tokenize(x)\n",
    "    #Remove punctuation\n",
    "    x = [w.translate(table) if not re.match(r'not_.*', w) else w for w in x]\n",
    "    print(x)\n",
    "    #Change numbers into words\n",
    "    x = [p.number_to_words(w) if w.isdigit() else w  for w in x ]\n",
    "    print(x)\n",
    "    #Remove not alphabetic\n",
    "    x = [w for w in x if w.isalpha() or re.match(r'not_.*',w)]\n",
    "    #Convert to lower case\n",
    "    x = [w.lower() for w in x]\n",
    "    print(x)\n",
    "    #Remove stop words\n",
    "    x = [w for w in x if not w in sr]\n",
    "    print(x)\n",
    "    ## lemmatization\n",
    "    x = [wnl.lemmatize(w) for w in x]\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
